{
    "contents" : "#######################################################################################################\n#\n#             DETECCIÓN DE INSULTOS USANDO UN CLASIFICADOR DE MÁXIMA ENTROPÍA \n#\n# Este código se divide en dos partes:\n#\n#     [1]: Predicción de un comentario \n#          - Se entrena el modelo usando toda la data de train.csv\n#          - Se usa una función que recibe como argumento un comentario ingresado por teclado \n#          - La función devuelve la probabilidad de que el comentario sea insulto\n#\n#     [2]: Validación Cruzada\n#          - Se entrena el modelo usando una muestra aleatoria igual a la mitad de train.csv \n#          - Se predice usando la otra mitad de la data\n#          - Se calcula la precisión\n#\n# Observaciones: - Precisión aproximada: 0.75 ~ Sin preprocesar la data\n#                - Solo considera la frecuencia de las palabras (por ahora)\n#                - Información de la sesión:\n#                                             R version 3.2.1 (2015-06-18)\n#                                             Platform: i686-pc-linux-gnu (32-bit)\n#                                             Running under: Ubuntu precise (12.04.5 LTS)\n#\n######################################################################################################\n\n#library(RTextTools)  # RTextTools_1.4.0\n#library(tm)          # tm_0.5-10\n#library(maxent)      # maxent_1.3.3.1\n#library(caret)       # caret_6.0-52\n\n### Gets the data\n\n#all.traindata <- read.csv('/home/obama/Desktop/MD/train.csv', row.names = NULL, stringsAsFactors = FALSE)\n#all.traindata$ID <- NULL\n\n#badwords <- read.table('/home/obama/Desktop/badwords.txt', sep = \"\\n\")\n#badwords <- unlist(badwords)\n\n# Silly function to round the probability\nis.insult <- function(probability){\n  if(probability > 0.65)\n    return(1)\n  else\n    return(0)\n}\n\n\n#####---------- [1] Predicts just a comment ----------#####\n\n\n### Makes corpora\n\ncorpus <- Corpus(VectorSource(cleanComm(all.traindata$Comment)))\n\n### Cleans corpora\n\n#corpus <- tm_map(corpus, removePunctuation)                  # THIS DOESNT AFFECT THE ACCURACY !!  \n#corpus <- tm_map(corpus, stripWhitespace)                    # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, content_transformer(tolower))       # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, removeWords, stopwords('english'))  # THIS REDUCES THE ACCURACY !! \n#corpus <- tm_map(corpus, stemDocument, lazy = TRUE)          # FUCKS THE DocumentTermMatrix !!\n\n### Builds a term-document matrix\n\nmatrix <- DocumentTermMatrix(corpus)\n# 10 most frequent words\nfindFreqTerms(matrix, 10)\n\n### Creates a MAXENT model [package: 'maxent']\n\nsparse <- as.compressed.matrix(matrix)\nmodelMAXENT <- maxent(sparse[1:nrow(all.traindata),], as.factor(all.traindata$Insult)[1:nrow(all.traindata)])\n\n### Predicts\n\n# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself\nprob.of.insult <- function(comment){\n  \n  # Prepares data to be predicted\n  testdata <- as.matrix(comment)\n  predCorpus <- Corpus(VectorSource(testdata))\n  predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))\n  predSparse <- as.compressed.matrix(predMatrix)\n\n  # Predicts\n  resultModel <- predict(modelMAXENT, predSparse[1:nrow(testdata),])\n  result <- as.numeric(resultModel[,3])\n  \n  # Returns the probability of be an insult\n  return(result)\n}\n\n### Reads from keyboard\n\ncomment <- readline(\"Enter your comment: \")\n\n### Calls the function with the comment typed as argument\n\nresultProb <- prob.of.insult(comment)\nresultProb\n\n# Prints INSULT or NO-INSULT \nif(is.insult(resultProb) == 1) print(\"INSULT\") else print(\"NO-INSULT\")\n\n\n#####---------- [2] Predicts a subset of all.traindata (cv: CROSS-VALIDATION) ----------#####\n\n\n# Function that split a data frame into two subsets of the same size\nsplitdf <- function(dataframe, seed=NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  index <- 1:nrow(dataframe)\n  trainindex <- sample(index, trunc(length(index)/2))\n  trainset <- dataframe[trainindex,]\n  testset <- dataframe[-trainindex,]\n  list(trainset = trainset, testset = testset)\n}\n\n# Calls the function and obtain train data and test data\nsplits <- splitdf(all.traindata, seed = 666)\ntraindata <- splits$trainset\ntestdata <- splits$testset\n\n### Makes corpora\n\ncorpus.cv <- Corpus(VectorSource(traindata$Comment))\n\n### Cleans corpora\n\n#corpus <- tm_map(corpus, removePunctuation)                  # THIS DOESNT AFFECT THE ACCURACY !!  \n#corpus <- tm_map(corpus, stripWhitespace)                    # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, content_transformer(tolower))       # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, removeWords, stopwords('english'))  # THIS REDUCES THE ACCURACY !! \n#corpus <- tm_map(corpus, stemDocument, lazy = TRUE)          # FUCKS THE DocumentTermMatrix !!\n\n### Builds a term-document matrix\n\nmatrix.cv <- DocumentTermMatrix(corpus.cv)\n# 10 most frequent words\n#findFreqTerms(matrix.cv, 10)\n\n### Creates a MAXENT model [package: 'maxent']\n\nsparse.cv <- as.compressed.matrix(matrix.cv)\nmodelMAXENT.cv <- maxent(sparse.cv[1:nrow(traindata),], as.factor(traindata$Insult)[1:nrow(traindata)])\n\n### Predicts\n\n# Prepares data to be predicted\npredCorpus.cv <- Corpus(VectorSource(testdata$Comment))\npredMatrix.cv <- DocumentTermMatrix(predCorpus.cv, list(dictionary = Terms(matrix.cv)))\npredSparse.cv <- as.compressed.matrix(predMatrix.cv)\n  \n# Predicts\nresultModel.cv <- predict(modelMAXENT.cv, predSparse.cv[1:nrow(testdata),])\nresult.cv <- cbind.data.frame(Comment = testdata, Probability = as.numeric(resultModel.cv[,3]))\nresult.cv$Class <- lapply(result.cv$Probability, is.insult)\n\n### Calculates accuracy and confusion matrix\n\nrecall_accuracy(testdata$Insult[1:nrow(testdata)], result.cv$Probability)\n\nconfusionMatrix(testdata$Insult, unlist(result.cv$Class))\n\n",
    "created" : 1439328889545.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "560664942",
    "id" : "776616EF",
    "lastKnownWriteTime" : 1439330434,
    "path" : "C:/Users/wilmer g/Desktop/DM/WarmChat/models/maxent_model.r",
    "project_path" : "models/maxent_model.r",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}