{
    "contents" : "#######################################################################################################\n#\n#             DETECCIÓN DE INSULTOS USANDO UN CLASIFICADOR DE MÁXIMA ENTROPÍA \n#\n# Este código se divide en dos partes:\n#\n#     [1]: Predicción de un comentario \n#          - Se entrena el modelo usando toda la data de train.csv\n#          - Se usa una función que recibe como argumento un comentario ingresado por teclado \n#          - La función devuelve la probabilidad de que el comentario sea insulto\n#\n#     [2]: Validación Cruzada\n#          - Se entrena el modelo usando una muestra aleatoria igual a la mitad de train.csv \n#          - Se predice usando la otra mitad de la data\n#          - Se calcula la precisión\n#\n# Observaciones: - Precisión aproximada: 0.75 ~ Sin preprocesar la data\n#                - Solo considera la frecuencia de las palabras (por ahora)\n#                - Información de la sesión:\n#                                             R version 3.2.1 (2015-06-18)\n#                                             Platform: i686-pc-linux-gnu (32-bit)\n#                                             Running under: Ubuntu precise (12.04.5 LTS)\n#\n######################################################################################################\n\n#library(RTextTools)  # RTextTools_1.4.0\n#library(tm)          # tm_0.5-10\n#library(maxent)      # maxent_1.3.3.1\n#library(caret)       # caret_6.0-52\n\n### Gets the data\n\n# load(\"/data/data.Rdata\")\n\n\n#####---------- [1] Predicts just a comment ----------#####\n\n\n### Makes corpora\n# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself\nmaxent_predict <-function(comm){\n  ### Training model\n  #corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))\n  \n  ### Builds a term-document matrix\n  \n  #matrix <- DocumentTermMatrix(corpus)\n  # 10 most frequent words\n  #findFreqTerms(matrix, 10)\n  \n  ### Creates a MAXENT model [package: 'maxent']\n  \n  #sparse <- as.compressed.matrix(matrix)\n  #modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])\n  #save(modelMAXENT,file = \"maxentFit.Rdata\")\n  load(\"maxentFit.Rdata\")\n  ### Predicts  \n  testdata <- as.matrix(comm)\n  predCorpus <- Corpus(VectorSource(testdata))\n  predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))\n  predSparse <- as.compressed.matrix(predMatrix)\n  \n  # Predicts\n  resultModel <- predict(modelMAXENT, predSparse[1:nrow(testdata),])\n  result <- as.numeric(resultModel[,3])\n  \n  if(result > 0.65){\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n# r <- c()\n# for(i in 1:length(train$Comment)){\n#\t r[i]=maxent_predict(as.character(train$Comment[i]))\n# }\n# train1 <-cbind(train,maxent =r)\n# table(train1$Insult,train1$maxent)\n# library(\"caret\")\n# confusionMatrix(train1$Insult,train1$maxent)\n\n#####---------- [2] Predicts a subset of all.traindata (cv: CROSS-VALIDATION) ----------#####\n\n\n# Function that split a data frame into two subsets of the same size\n#splitdf <- function(dataframe, seed=NULL) {\n#  if (!is.null(seed)) set.seed(seed)\n#  index <- 1:nrow(dataframe)\n#  trainindex <- sample(index, trunc(length(index)/2))\n#  trainset <- dataframe[trainindex,]\n#  testset <- dataframe[-trainindex,]\n#  list(trainset = trainset, testset = testset)\n#}\n\n# Calls the function and obtain train data and test data\n#splits <- splitdf(all.traindata, seed = 666)\n#traindata <- splits$trainset\n#testdata <- splits$testset\n\n### Makes corpora\n\n#corpus.cv <- Corpus(VectorSource(traindata$Comment))\n\n### Cleans corpora\n\n#corpus <- tm_map(corpus, removePunctuation)                  # THIS DOESNT AFFECT THE ACCURACY !!  \n#corpus <- tm_map(corpus, stripWhitespace)                    # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, content_transformer(tolower))       # THIS DOESNT AFFECT THE ACCURACY !!\n#corpus <- tm_map(corpus, removeWords, stopwords('english'))  # THIS REDUCES THE ACCURACY !! \n#corpus <- tm_map(corpus, stemDocument, lazy = TRUE)          # FUCKS THE DocumentTermMatrix !!\n\n### Builds a term-document matrix\n\n#matrix.cv <- DocumentTermMatrix(corpus.cv)\n# 10 most frequent words\n#findFreqTerms(matrix.cv, 10)\n\n### Creates a MAXENT model [package: 'maxent']\n\n#sparse.cv <- as.compressed.matrix(matrix.cv)\n#modelMAXENT.cv <- maxent(sparse.cv[1:nrow(traindata),], as.factor(traindata$Insult)[1:nrow(traindata)])\n\n### Predicts\n\n# Prepares data to be predicted\n#predCorpus.cv <- Corpus(VectorSource(testdata$Comment))\n#predMatrix.cv <- DocumentTermMatrix(predCorpus.cv, list(dictionary = Terms(matrix.cv)))\n#predSparse.cv <- as.compressed.matrix(predMatrix.cv)\n  \n# Predicts\n#resultModel.cv <- predict(modelMAXENT.cv, predSparse.cv[1:nrow(testdata),])\n#result.cv <- cbind.data.frame(Comment = testdata, Probability = as.numeric(resultModel.cv[,3]))\n#result.cv$Class <- lapply(result.cv$Probability, is.insult)\n\n### Calculates accuracy and confusion matrix\n\n#recall_accuracy(testdata$Insult[1:nrow(testdata)], result.cv$Probability)\n\n#confusionMatrix(testdata$Insult, unlist(result.cv$Class))\n\n",
    "created" : 1439328889545.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "366711597",
    "id" : "776616EF",
    "lastKnownWriteTime" : 1439335965,
    "path" : "C:/Users/wilmer g/Desktop/DM/WarmChat/models/maxent_model.r",
    "project_path" : "models/maxent_model.r",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}