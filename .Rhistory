#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
#testdata <- as.matrix(comm)
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus)
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.65){
return(1)
} else {
return(0)
}
}
r <- c()
for(i in 1:length(train$Comment)){
r[i]=maxent_predict(as.character(train$Comment[i]))
}
r
r <- c()
for(i in 1:length(train$Comment)){
r[i]=maxent_predict(as.character(train$Comment[i]))
}
unique(r)
tail(r)
View(resultModel)
train1 <-cbind(train,maxent =r)
maxent_predict("i love u")
maxent_predict("i love ")
maxent_predict("i love mi son")
### Makes corpora
# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
load("maxentFit.Rdata")
### Predicts
testdata <- as.matrix(comm)
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.65){
return(1)
} else {
return(0)
}
}
maxent_predict("i love mi son")
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- as.matrix(comm)
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.65){
return(1)
} else {
return(0)
}
}
maxent_predict("i love mi son")
maxent_predict("i love u")
maxent_predict("i love")
maxent_predict("i")
maxent_predict("i love kye")
maxent_predict("i love kilo")
maxent_predict("u asshole")
maxent_predict("u kind")
maxent_predict("u kind je")
maxent_predict(" kind je")
maxent_predict("key hore")
maxent_predict("key ")
maxent_predict("key whore")
maxent_predict("idio")
maxent_predict("idiot mother")
maxent_predict("idiot i ove uoy")
maxent_predict(" dont idiot")
maxent_predict(" i love u love ")
# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- as.matrix(comm)
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.8){
return(1)
} else {
return(0)
}
}
maxent_predict(" i love u love ")
maxent_predict(" dont idiot")
maxent_predict(" i love u")
maxent_predict(" i care about u")
maxent_predict(" shame of u")
r <- c()
r <- c()
for(i in 1:length(train$Comment)){
r[i]=maxent_predict(as.character(train$Comment[i]))
}
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
#testdata <- as.matrix(comm)
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.8){
return(1)
} else {
return(0)
}
}
maxent_predict(" shame of u")
maxent_predict(" i care about u")
maxent_predict(" i care about u")
maxent_predict(" i care about u")
maxent_predict(" shame of u")
maxent_predict(" dont idiot")
maxent_predict("idiot i ove uoy")
maxent_predict("love love i ove ")
# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- comm
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.8){
return(1)
} else {
return(0)
}
}
maxent_predict("love love i ove ")
maxent_predict("love love ")
maxent_predict("love you mother")
rm("comm")
# Function to predict a comment ~ it needs the dtm used to calculate the max-ent model and the max-ent model itself
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- comm
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.8){
return(1)
} else {
return(0)
}
}
maxent_predict("love you mother")
maxent_predict("love you my child")
rm("testdata")
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- comm
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.8){
return(1)
} else {
return(0)
}
}
maxent_predict("love you my child")
maxent_predict("love love love")
maxent_predict("you my favorite guy")
maxent_predict("you my favorite guy son of ")
maxent_predict("you are the worst person that i know")
maxent_predict("you are the best person that i know")
maxent_predict("you are the idiot person that i know")
maxent_predict("you are the greatest person that i know")
maxent_predict("you are the greatest person that i know")
maxent_predict("you are the greatesdf sdf sdf")
maxent_predict("you are the asshole greatesdf sdf sdf")
maxent_predict("you are an asshole")
maxent_predict("you arent an asshole")
maxent_predict("you are not an asshole")
maxent_predict("youre not an asshole")
maxent_predict("youre not an assho")
maxent_predict("i like you")
maxent_predict("i love you")
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- comm
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.9){
return(1)
} else {
return(0)
}
}
maxent_predict("i love you")
maxent_predict("i like you")
maxent_predict("i kinda like you")
maxent_predict("you dont the smartest person")
maxent_predict("you are not the smartest person")
maxent_predict("you are the smartest person")
maxent_predict("you are the worst")
setwd("data")
load("data.Rdata")
setwd("..")
nrow(subset(train,subset = train$Insult == 1))
insultt<- subset(train,subset = train$Insult == 1)
safe <- subset(train,subset = train$Insult == 0)
safe <- safe[1:2501]
safe <- safe[1:2501,]
safe <- subset(train,subset = train$Insult == 0)
safe <- safe[1:3000,]
safe <- subset(train,subset = train$Insult == 0)
safe <- safe[1:3500,]
train <- rbind(insultt,safe)
getwd()
setwd("data")
save(train,badwords,"data.Rdata")
save(train,badwords,file = "data.Rdata")
rm("safe","insultt")
load("models/maxentFit.Rdata")
setwd("..")
load("models/maxentFit.Rdata")
maxent_predict <-function(comm){
### Training model
#corpus <- Corpus(VectorSource(cleanComm(as.character(train$Comment))))
### Builds a term-document matrix
#matrix <- DocumentTermMatrix(corpus)
# 10 most frequent words
#findFreqTerms(matrix, 10)
### Creates a MAXENT model [package: 'maxent']
#sparse <- as.compressed.matrix(matrix)
#modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
#save(modelMAXENT,file = "maxentFit.Rdata")
### Predicts
testdata <- comm
predCorpus <- Corpus(VectorSource(testdata))
predMatrix <- DocumentTermMatrix(predCorpus, list(dictionary = Terms(matrix)))
predSparse <- as.compressed.matrix(predMatrix)
# Predicts
resultModel <- predict(modelMAXENT, predSparse[1,])
result <- as.numeric(resultModel[,3])
if(result > 0.9){
return(1)
} else {
return(0)
}
}
r <- c()
for(i in 1:length(train$Comment)){
r[i]=maxent_predict(as.character(train$Comment[i]))
}
rm("i","r")
View(train)
enc2utf8(train$Comment[11,])
train$Comment[11,]
train$Comment[11,]
enc2utf8(train$Comment[11])
train$Comment[11]
train$Comment[11]
as.character(train$Comment[11])
as.character(train$Comment)[11]
as.character(train$Comment)[16]
as.character(train$Comment)[7]
enc2utf8(as.character(train$Comment)[7])
grep("\xa0",as.character(train$Comment),fixed = T)
grep("\xa0",train$Comment,fixed = T)
grep("\\xa0",train$Comment,fixed = T)
as.character(train$Comment)[2]
cat(paste(as.character(train$Comment), collapse="\n"), file=file.path("bad", paste0("bad", ".txt")))
cat(paste(as.character(train$Comment), collapse="\n"), file=file.path(paste0("bad", ".txt")))
write.csv(train,file= "bad.csv",header = T,quote=T,sep = ",")
write.csv(train,file= "bad.csv",quote=T)
training <- read.csv(file = "bad.csv")
View(training)
training <- training[,-1]
train <- training
rm("training")
train1$Insult <- train$Insult
train<- c()
train1$Insult <- train$Insult
train1<- c()
train1$Insult <- train$Insult
train1<- data.frame
train1<- data.frame()
train1$Insult <- train$Insult
train1$Insult
train <-read.csv(file = "bad.csv")
train<- train[,-1]
train1$Insult <- train$Insult
train1<- train
cleanSet<- function(word){
## @knitr encoding_input
enc2utf8(word)
## @knitr removing_html
gsub("<.*?>", "", word)
## @knitr removing_urls
gsub("http[[:alnum:][:punct:]]*", "", word)
## @knir removing_rep
gsub('([[:alpha:]])\\2+', '\\2', word)
## @knitr to_lowering
word<- tolower(word)
## @knitr removing_punctuations
word<- removePunctuation(word)
## @knitr removing_number
word <- removeNumbers(word)
## @knitr removing_whitespaces
wordn <- stripWhitespace(word)
return(wordn)
}
train1$Comment <- cleanSet(train1$Comment)
train1$Comment <- cleanSet(as.character(train1$Comment))
str <- "loooolllllll"
paste(rle(strsplit(str, "")[[1]])$values, collapse="")
paste(rle(strsplit(str, "")[[2]])$values, collapse="")
?rle
cleanSet<- function(word){
## @knitr encoding_input
enc2utf8(word)
## @knitr removing_html
gsub("<.*?>", "", word)
## @knitr removing_urls
gsub("http[[:alnum:][:punct:]]*", "", word)
## @knir removing_rep
gsub('([[:alpha:]])\\2+', '\\1', word)
## @knitr to_lowering
word<- tolower(word)
## @knitr removing_punctuations
word<- removePunctuation(word)
## @knitr removing_number
word <- removeNumbers(word)
## @knitr removing_whitespaces
wordn <- stripWhitespace(word)
return(wordn)
}
str <- "loooolllllll"
cleanSet(str)
# Preprocesamiento de los datos
cleanSet<- function(word){
## @knitr encoding_input
enc2utf8(word)
## @knitr removing_html
gsub("<.*?>", "", word)
## @knitr removing_urls
gsub("http[[:alnum:][:punct:]]*", "", word)
## @knir removing_rep
gsub('([[:alpha:]])\\1+', '\\1', word)
## @knitr to_lowering
word<- tolower(word)
## @knitr removing_punctuations
word<- removePunctuation(word)
## @knitr removing_number
word <- removeNumbers(word)
## @knitr removing_whitespaces
wordn <- stripWhitespace(word)
return(wordn)
}
cleanSet(str)
cleanSet<- function(word){
## @knitr encoding_input
enc2utf8(word)
## @knitr removing_html
gsub("<.*?>", "", word)
## @knitr removing_urls
gsub("http[[:alnum:][:punct:]]*", "", word)
## @knir removing_rep
word <- paste(rle(strsplit(word, "")[[1]])$values, collapse="")
## @knitr to_lowering
word<- tolower(word)
## @knitr removing_punctuations
word<- removePunctuation(word)
## @knitr removing_number
word <- removeNumbers(word)
## @knitr removing_whitespaces
wordn <- stripWhitespace(word)
return(wordn)
}
cleanSet(str)
setwd("data")
save(train,badwords,file = "data.Rdata")
setwd("..")
rm("train1","str")
corpus <- Corpus(VectorSource(cleanSet(as.character(train$Comment))))
matrix <- DocumentTermMatrix(corpus)
sparse <- as.compressed.matrix(matrix)
modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train)])
modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train),])
modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train),])
sparse <- as.compressed.matrix(matrix)
modelMAXENT <- maxent(sparse[1:nrow(train),], as.factor(train$Insult)[1:nrow(train),])
?maxent
train$Comment
cleanSet(as.character(train$Comment))
train1 <- train
for(i in 1:nrow(train1)){
train1$Comment[i,] <- cleanSet(train1$Comment[i,])
}
for(i in 1:nrow(train1)){
train1$Comment[i] <- cleanSet(train1$Comment[i])
}
for(i in 1:nrow(train1)){
train1$Comment[i] <- cleanSet(as.character(train1$Comment[i]))
}
warnings()
View(train1)
for(i in 1:nrow(train1)){
train1$Comment[i] <- cleanSet(as.character(train$Comment[i]))
}
